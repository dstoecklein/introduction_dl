{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7332a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff1c32",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00652e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "    \n",
    "    trans = [transforms.ToTensor()]\n",
    "    \n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "        \n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../datasets\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../datasets\", train=False, transform=trans, download=True)\n",
    "    \n",
    "    return (\n",
    "                data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=4),\n",
    "                data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=4)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01a42edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de98149",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a8a403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008edc6d",
   "metadata": {},
   "source": [
    "The output layer of softmax regression is a fully-connected layer. Therefore, to implement our model, we just need to add one fully-connected layer with 10 outputs to our Sequential. Again, here, the Sequential is not really necessary, but we might as well form the habit since it will be ubiquitous when implementing deep models. Again, we initialize the weights at random with zero mean and standard deviation 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b479766",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "        \n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf0e0e",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f98549f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f187b9",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27b67ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten(start_dim=1, end_dim=-1)\n",
      "Linear(in_features=784, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "549b8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a5d99",
   "metadata": {},
   "source": [
    "## Model training\n",
    "* initiate randomly the linear model parameters $(\\mathbf{w}, b)$\n",
    "* repeat for each epoch (unitl done)\n",
    "    * load batch of data\n",
    "    * do linear model precitions with current parameter values\n",
    "    * calculate loss\n",
    "    * get gradients\n",
    "    * update parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "def train_epoch_ch3(net, train_iter, loss, updater):  #@save\n",
    "    \"\"\"The training loop defined in Chapter 3.\"\"\"\n",
    "    # Set the model to training mode\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    # Sum of training loss, sum of training accuracy, no. of examples\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # Compute gradients and update parameters\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # Using PyTorch in-built optimizer & loss criterion\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            updater.step()\n",
    "            metric.add(float(l) * len(y), accuracy(y_hat, y), y.numel())\n",
    "        else:\n",
    "            # Using custom built optimizer & loss criterion\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    # Return training loss and training accuracy\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
