{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c648ee86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0868e8c",
   "metadata": {},
   "source": [
    "### Basics\n",
    "- Let's assume 2x2 images = 4 features\n",
    "- *One-hot encoding*: Vector with as many components as we have categories. The component corresponding to particular instance's category ist set to 1 and all other are set to 0. Label $y$ has 3 categories: cat, chicken, dog\n",
    "\n",
    "$$y \\in \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\}.$$\n",
    "\n",
    "- We need a model with multiple outputs -> **one per class**\n",
    "- 4 Features and 3 categories = 12 weight scalars and 3 bias scalars\n",
    "$$\n",
    "\\begin{aligned}\n",
    "o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\n",
    "o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\n",
    "o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Full connected Layers:\n",
    "![softmax](softmaxreg.svg)\n",
    "- Express model more compact:\n",
    "$\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$\n",
    "- - $\\mathbf{W}$: 3x4 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303226f",
   "metadata": {},
   "source": [
    "### Approach\n",
    "- We interpret the outputs as probabilities\n",
    "- - They must sum to 1\n",
    "- Optimization: Maximize Likelihood\n",
    "- **Softmax**: Does exactly this: Transform logits such that they become non-negative and sum to 1\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}. $$\n",
    "\n",
    "- So $\\hat{y}_1 + \\hat{y}_2 + \\hat{y}_3 = 1$ Thus, $\\hat{\\mathbf{y}}$ is a proper probability distribution whose element values can be interpreted accordingly. exp wird also durch die Anzahl exp geteilt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba73071",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Vectorization of Minibatches\n",
    "- To improve computational efficiency\n",
    "- We are given a minibatch $\\mathbf{X}$ of examples with feature dimensionality $d$ (number of inputs) and batch size $n$.\n",
    "- We have $q$ categories in the output\n",
    "- - Then the minibatch features $\\mathbf{X}$ are in $\\mathbb{R}^{n \\times d}$\n",
    "- - weights $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$\n",
    "- - bias satisfies $\\mathbf{b} \\in \\mathbb{R}^{1\\times q}$\n",
    "\n",
    "$$ \\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89886f01",
   "metadata": {},
   "source": [
    "### Loss Function (Cross Entropy Loss)\n",
    "- To measure the quality of our predicted probabilites.\n",
    "- $\\hat{\\mathbf{y}}$ gives us a vector of **conditional** probabilities nof each class given any input $\\mathbf{x}$. \n",
    "- - $\\hat{y}_1$ = $P(y=\\text{cat} \\mid \\mathbf{x})$\n",
    "- Suppose that the entire dataset $\\{\\mathbf{X}, \\mathbf{Y}\\}$ has $n$ examples, where the example indexed by $i$ consists of a feature vector $\\mathbf{x}^{(i)}$ and a one-hot label vector $\\mathbf{y}^{(i)}$. We can compare the estimates with reality by checking how probable the actual classes are according to our model, given the features:\n",
    "\n",
    "- Probabilities of entire Dataset:\n",
    "$$\n",
    "P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).\n",
    "$$\n",
    "\n",
    "- - ($ P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).$ = Probabilities of 1 Sample)\n",
    "\n",
    "- According to maximum likelihood estimation, we maximize $P(\\mathbf{Y} \\mid \\mathbf{X})$, which is equivalent to minimizing the negative log-likelihood.\n",
    "- where for any pair of label $\\mathbf{y}$ and model prediction $\\hat{\\mathbf{y}}$ over $q$ classes, the loss function $l$ is\n",
    "\n",
    "$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
