{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff10ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa0718",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c8106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "    \n",
    "    trans = [transforms.ToTensor()]\n",
    "    \n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "        \n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../datasets\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../datasets\", train=False, transform=trans, download=True)\n",
    "    \n",
    "    return (\n",
    "                data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=4),\n",
    "                data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=4)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703c2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee992b2",
   "metadata": {},
   "source": [
    "- number of features: 28x28px = 784\n",
    "- number of outputs: 10 categories\n",
    "- weigths: 784x10 matrix\n",
    "- bias: 1x10 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c17936",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b02ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs,  requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bc69b3",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "- (i) we exponentiate each term (using exp)\n",
    "- (ii) we sum over each row (we have one row per example in the batch) to get the normalization constant for each example\n",
    "- (iii) we divide each row by its normalization constant, ensuring that the result sums to 1.\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}. $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e49eb9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a786b6a",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "- Note that we flatten each original image in the batch into a vector using the reshape function before passing the data through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45becfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72a4c3",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c89edd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.3000, 0.6000],\n",
       "        [0.3000, 0.2000, 0.5000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdef76d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y])\n",
    "cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb84d6",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "- To compute accuracy we do the following. First, if y_hat is a matrix, we assume that the second dimension stores prediction scores for each class. We use argmax to obtain the predicted class by the index for the largest entry in each row. Then we compare the predicted class with the ground-truth y elementwise. Since the equality operator == is sensitive to data types, we convert y_hat’s data type to match that of y. The result is a tensor containing entries of 0 (false) and 1 (true). Taking the sum yields the number of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b033df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424de266",
   "metadata": {},
   "source": [
    "We will continue to use the variables y_hat and y defined before as the predicted probability distributions and labels, respectively. We can see that the first example’s prediction class is 2 (the largest element of the row is 0.6 with the index 2), which is inconsistent with the actual label, 0. The second example’s prediction class is 2 (the largest element of the row is 0.5 with the index of 2), which is consistent with the actual label, 2. Therefore, the classification accuracy rate for these two examples is 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "717542c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_hat, y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da40f94",
   "metadata": {},
   "source": [
    "Similarly, we can evaluate the accuracy for any model net on a dataset that is accessed via the data iterator data_iter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a70d35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):  #@save\n",
    "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52798559",
   "metadata": {},
   "source": [
    "Here Accumulator is a utility class to accumulate sums over multiple variables. In the above evaluate_accuracy function, we create 2 variables in the Accumulator instance for storing both the number of correct predictions and the number of predictions, respectively. Both will be accumulated over time as we iterate over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1736e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:  #@save\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34972f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0587"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6594b5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc870e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):  #@save\n",
    "    \"\"\"The training loop defined in Chapter 3.\"\"\"\n",
    "    # Set the model to training mode\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    # Sum of training loss, sum of training accuracy, no. of examples\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # Compute gradients and update parameters\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # Using PyTorch in-built optimizer & loss criterion\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            updater.step()\n",
    "            metric.add(float(l) * len(y), accuracy(y_hat, y), y.numel())\n",
    "        else:\n",
    "            # Using custom built optimizer & loss criterion\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    # Return training loss and training accuracy\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edfbbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
