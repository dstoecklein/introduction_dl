{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f0e3cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MLP\n",
    "\n",
    "## Steps to do:\n",
    "\n",
    "1. get training data\n",
    "2. prepare data loading\n",
    "3. define model\n",
    "4. define optimization procedure\n",
    "5. train model (and evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e973148",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MNIST dataset\n",
    "\n",
    "Standard benchmark dataset for image classification - hand written digits: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "We will use `torchvision.datasets` to load the data and explore its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07377382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test MNIST data\n",
    "# --- YOUR CODE HERE ---\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "mnist_train = datasets.MNIST('../Datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST('../Datasets', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec175a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data loading\n",
    "\n",
    "Use `torch.uitls.data.DataLoader` to prepare the batch loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60fb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct train and test data loaders to provide batches of data\n",
    "# --- YOUR CODE HERE ---\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 10\n",
    "dl_train = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "dl_test = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be18888",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model definition\n",
    "\n",
    "2-layer MLP with ReLU activation:\n",
    "\n",
    "$$\\mathbf{h} = \\sigma(\\mathbf{x}^T \\mathbf{W}{(1)} + \\mathbf{b}{(1)}) \\\\\n",
    "\\mathbf{o} = \\mathbf{h}^T \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223e5b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize parametes\n",
    "# --- YOUR CODE HERE ---\n",
    "import torch.nn as nn\n",
    "\n",
    "num_inputs, num_outputs, num_hidden = 784, 10, 16\n",
    "\n",
    "w1 = torch.nn.Parameter(torch.randn(num_inputs, num_hidden))\n",
    "b1 = torch.nn.Parameter(torch.randn(num_hidden))\n",
    "w2 = torch.nn.Parameter(torch.randn(num_hidden, num_outputs))\n",
    "b2 = torch.nn.Parameter(torch.randn(num_outputs))\n",
    "\n",
    "\n",
    "w1.requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceaf0da5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.6581, 0.7088],\n",
       "        [0.3933, 0.0000, 0.2369],\n",
       "        [0.6248, 0.3164, 0.0000],\n",
       "        [1.0337, 0.5241, 0.0000],\n",
       "        [2.6988, 0.0000, 0.5725]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define relu activation function ReLU(x) = max(x, 0)\n",
    "# --- YOUR CODE HERE ---\n",
    "def relu(x):\n",
    "    return torch.maximum(x,torch.Tensor([0]))\n",
    "\n",
    "mt =torch.randn(5,3)\n",
    "\n",
    "relu(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe6c981",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the MLP model\n",
    "# --- YOUR CODE HERE ---\n",
    "\n",
    "\n",
    "\n",
    "def lin_reg(x, w, b):\n",
    "    return x.matmul(w) + b\n",
    "\n",
    "def mlp(x):\n",
    "    # 1st linear layer\n",
    "    h = lin_reg(x, w1, b1)\n",
    "    # pass through relu\n",
    "    o = relu(h)\n",
    "    # 2nd linear layer\n",
    "    y = lin_reg(o, w2, b2)\n",
    "    \n",
    "    return y\n",
    "\n",
    "x = torch.randn(10, num_inputs)\n",
    "y = mlp(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6abdf65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model training\n",
    "\n",
    "Follows the same logic as for linear regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd07231f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2847/3629427388.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# get loss using predictions and true targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# get gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/idl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/idl/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/miniconda3/envs/idl/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "# write the training procedure\n",
    "# --- YOUR CODE HERE ---\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.1\n",
    "num_epochs = 1\n",
    "batch_size = 256\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD([w1, b1, w2, b2], lr)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    for count, (x_batch, y_batch) in enumerate(mnist_train):  #train_dl\n",
    "        \n",
    "        # predictions using our mlp\n",
    "        y_hat = mlp(torch.flatten(x_batch, 1)) #y_hat = mlp(torch.flatten(x_batch, 1), w1, b1, w2, b2)\n",
    "\n",
    "        # get loss using predictions and true targets\n",
    "        ll = loss(y_hat, y_batch)\n",
    "\n",
    "        # get gradients\n",
    "        optimizer.zero_grad()\n",
    "        ll.backward()\n",
    "        if count //20 == 0:\n",
    "            losses.append(ll.item())\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7888a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot graphs\n",
    "&matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950b005",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Display 10 data examples together with their labels\n",
    "\n",
    "fig, axs = plt.subplots(2, 5)\n",
    "fig.set_size_inches(8, 4)\n",
    "axs = axs.flatten()\n",
    "for i, batch in enumerate(mnist_test):\n",
    "    axs[i].imshow(batch[0].squeeze(), cmap='gray')\n",
    "    preds = mlp(torch.flatten(batch[0], 1))\n",
    "    preds = torch.argmax(preds, dim=1)\n",
    "    axs[i].set_title(f\"{batch[1]}/{preds.item()}\")\n",
    "    axs[i].xaxis.set_visible(False)\n",
    "    axs[i].yaxis.set_visible(False)\n",
    "    if i == 9:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
