{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7xeWfGbVVHX"
      },
      "source": [
        "## Authors:\n",
        "#### Daniel St√∂ckein (5018039), Alexander Triol (5018451)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpJ112ZGVVHa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiuFMWJyVVHc"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymbiw3axVVHd"
      },
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQl0ekgJVVHd"
      },
      "outputs": [],
      "source": [
        "mnist_train = datasets.FashionMNIST(\n",
        "    root='datasets/', \n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "mnist_test = datasets.FashionMNIST(\n",
        "    root='datasets/', \n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=transforms.ToTensor()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUqpIBPuVVHe"
      },
      "source": [
        "## 2. Preparing DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE-sCYgWVVHf"
      },
      "outputs": [],
      "source": [
        "def dloaders(batch_size):\n",
        "    train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m--qgfyIVVHf"
      },
      "source": [
        "## 3. Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMSr3zssgh5C"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_channels, fc_output1, fc_output2):\n",
        "        super(Residual, self).__init__()\n",
        "        self.lin1 = nn.Linear(input_channels, fc_output1)\n",
        "        self.lin2 = nn.Linear(fc_output1, fc_output2)\n",
        "        self.lin3 = nn.Linear(input_channels, fc_output2)\n",
        "        \n",
        "\n",
        "        self.rel1 = nn.ReLU()\n",
        "        self.rel2 = nn.ReLU()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(input_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(fc_output1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = self.bn1(X)\n",
        "        Y = self.rel1(Y)\n",
        "        Y = F.dropout(Y, p=0.5)\n",
        "        Y = self.lin1(Y)\n",
        "        Y = self.bn2(Y)\n",
        "        Y = self.rel2(Y)\n",
        "        Y = self.lin2(Y)\n",
        "        Y += self.lin3(X)\n",
        "        return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RkUsXWzVVHh"
      },
      "source": [
        "## 4. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POHPz4ehVVHh"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_features, fc_output1, fc_output2, outputs):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = nn.Sequential(nn.Flatten(), \n",
        "                          nn.Linear(input_features, 256),\n",
        "                          Residual(256, fc_output1, fc_output2),\n",
        "                          nn.BatchNorm1d(fc_output2),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(fc_output2, 64),\n",
        "                          Residual(64, fc_output1, fc_output2),\n",
        "                          nn.BatchNorm1d(fc_output2),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(fc_output2, outputs),\n",
        "                          )\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = self.net(X)\n",
        "        return Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIs8M-J_VVHi"
      },
      "outputs": [],
      "source": [
        "model = MLP(784, 120, 84, 10)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE1YH7I3VVHi"
      },
      "source": [
        "## 5. Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw0iicynVVHi"
      },
      "outputs": [],
      "source": [
        "def comp_accuracy(model, data_loader):\n",
        "    correct = 0\n",
        "    wrong = 0\n",
        "    num_examples = 0\n",
        "    \n",
        "    # turn on eval mode if model Inherits from nn.Module\n",
        "    if isinstance(model, nn.Module):\n",
        "        model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_index, (features, labels) in enumerate(data_loader):\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            _, predictions = torch.max(logits, dim=1) # single class with highest probability. simply retain indices\n",
        "\n",
        "            num_examples += labels.size(0)\n",
        "\n",
        "            correct += (predictions == labels).sum().float()\n",
        "            wrong += (predictions != labels).sum().float()\n",
        "            \n",
        "        accuracy = correct / num_examples * 100      \n",
        "        \n",
        "    return correct, wrong, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3LL2HIlVVHj"
      },
      "source": [
        "## 6. Training procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WYWQC0OVVHj"
      },
      "outputs": [],
      "source": [
        "def fit(model, train_loader, epochs, learning_rate, loss_func=nn.CrossEntropyLoss(), opt_func=torch.optim.SGD):\n",
        "    \n",
        "    optimizer = opt_func(model.parameters(), learning_rate) # objective function\n",
        "    model = model.to(device)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        model = model.train()\n",
        "              \n",
        "        for batch_index, (features, labels) in enumerate(train_loader):\n",
        "            \n",
        "            # gpu usage if possible\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # 1. forward\n",
        "            logits = model(features)\n",
        "\n",
        "            # 2. compute objective function (softmax, cross entropy)\n",
        "            cost = loss_func(logits, labels)\n",
        "\n",
        "            # 3. cleaning gradients\n",
        "            optimizer.zero_grad() \n",
        "\n",
        "            # 4. accumulate partial derivatives\n",
        "            cost.backward() \n",
        "\n",
        "            # 5. step in the opposite direction of the gradient\n",
        "            optimizer.step() \n",
        "            \n",
        "            if not batch_index % 250:\n",
        "                print ('Epoch: {}/{} | Batch {}/{} | Cost: {:.4f}'.format(\n",
        "                    epoch+1,\n",
        "                    epochs,\n",
        "                    batch_index,\n",
        "                    len(train_loader),\n",
        "                    cost\n",
        "                ))\n",
        "        \n",
        "        correct, wrong, accuracy = comp_accuracy(model, train_loader)\n",
        "        print ('Training: Correct[{:.0f}] | Wrong[{:.0f}] | Accuracy[{:.2f}%]'.format(\n",
        "            correct,\n",
        "            wrong,\n",
        "            accuracy\n",
        "        ), '\\n')\n",
        "\n",
        "    correct, wrong, accuracy = comp_accuracy(model, test_loader)  \n",
        "    print ('Test: Correct[{:.0f}] | Wrong[{:.0f}] | Accuracy[{:.2f}%]'.format(\n",
        "        correct,\n",
        "        wrong,\n",
        "        accuracy\n",
        "    ), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nP8h1OkVVHj"
      },
      "source": [
        "## 1. Attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_gL-SSaVVHk"
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "model = MLP(784, 200, 100, 10)\n",
        "train_loader, test_loader = dloaders(batch_size=batch_size) # data iters\n",
        "fit(model, train_loader, epochs, learning_rate) # training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCa6fL8wzggw"
      },
      "source": [
        "### Summary 1. Attempt with Problem 1 - Part B Parameters\n",
        "\n",
        "With the hyper parameters from Problem 1 - Part B, our model achieves an accuracy of ~86 percent. It therefore provides approx. 4 percent better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieyDie-ndvgB"
      },
      "source": [
        "## 2. Attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DEqvaNzjgUI"
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "epochs = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loader, test_loader = dloaders(batch_size=batch_size) # data iters\n",
        "model = MLP(784, 200, 100, 10)\n",
        "fit(model, train_loader, epochs, learning_rate) # training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GknKXKr509Ys"
      },
      "source": [
        "### Summary 2. Attempt, testing different parameters\n",
        "After playing with the hyper-parameters we have found that these hyper-parameters:\n",
        "*   batch_size = 50\n",
        "*   epochs = 10\n",
        "*   learning_rate = 0.1\n",
        "\n",
        "gave us the best accuracy of ~88 percent.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MLP (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}