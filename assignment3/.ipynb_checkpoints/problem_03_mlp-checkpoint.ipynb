{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7xeWfGbVVHX"
   },
   "source": [
    "## Authors:\n",
    "#### Daniel St√∂ckein (5018039), Alexander Triol (5018451)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LpJ112ZGVVHa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TiuFMWJyVVHc"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymbiw3axVVHd"
   },
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zQl0ekgJVVHd"
   },
   "outputs": [],
   "source": [
    "mnist_train = datasets.FashionMNIST(\n",
    "    root='../datasets/', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "mnist_test = datasets.FashionMNIST(\n",
    "    root='../datasets/', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transforms.ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUqpIBPuVVHe"
   },
   "source": [
    "## 2. Preparing DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gE-sCYgWVVHf"
   },
   "outputs": [],
   "source": [
    "def dloaders(batch_size):\n",
    "    train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m--qgfyIVVHf"
   },
   "source": [
    "## 3. Residual Block\n",
    "- A custom class from nn.Module will be created and in the ``forward`` function the data will pass through various layers according to the diagram\n",
    "- The following code implements the residual block with skip connections such that the input passed via the shortcut matches the dimensions of the main path's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CMSr3zssgh5C"
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, fc_output1, fc_output2):\n",
    "        super(Residual, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_channels, fc_output1)\n",
    "        self.lin2 = nn.Linear(fc_output1, fc_output2)\n",
    "        self.lin3 = nn.Linear(input_channels, fc_output2)\n",
    "        \n",
    "\n",
    "        self.rel1 = nn.ReLU()\n",
    "        self.rel2 = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(input_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(fc_output1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.bn1(X)\n",
    "        Y = self.rel1(Y)\n",
    "        Y = F.dropout(Y, p=0.5)\n",
    "        Y = self.lin1(Y)\n",
    "        Y = self.bn2(Y)\n",
    "        Y = self.rel2(Y)\n",
    "        Y = self.lin2(Y)\n",
    "        Y += self.lin3(X)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RkUsXWzVVHh"
   },
   "source": [
    "## 4. Model\n",
    "Data reminder:\n",
    "- 28x28 = 784 pixel values\n",
    "- 10 classes\n",
    "- 256 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "POHPz4ehVVHh"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_features, fc_output1, fc_output2, outputs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Flatten(), \n",
    "                          nn.Linear(input_features, 256),\n",
    "                          Residual(256, fc_output1, fc_output2),\n",
    "                          nn.BatchNorm1d(fc_output2),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(fc_output2, 64),\n",
    "                          Residual(64, fc_output1, fc_output2),\n",
    "                          nn.BatchNorm1d(fc_output2),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(fc_output2, outputs),\n",
    "                          )\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.net(X)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rIs8M-J_VVHi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (2): Residual(\n",
      "      (lin1): Linear(in_features=256, out_features=120, bias=True)\n",
      "      (lin2): Linear(in_features=120, out_features=84, bias=True)\n",
      "      (lin3): Linear(in_features=256, out_features=84, bias=True)\n",
      "      (rel1): ReLU()\n",
      "      (rel2): ReLU()\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BatchNorm1d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=84, out_features=64, bias=True)\n",
      "    (6): Residual(\n",
      "      (lin1): Linear(in_features=64, out_features=120, bias=True)\n",
      "      (lin2): Linear(in_features=120, out_features=84, bias=True)\n",
      "      (lin3): Linear(in_features=64, out_features=84, bias=True)\n",
      "      (rel1): ReLU()\n",
      "      (rel2): ReLU()\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BatchNorm1d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(784, 120, 84, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE1YH7I3VVHi"
   },
   "source": [
    "## 5. Metrics\n",
    "- computes accuracy and returns ``list(correct, wrong, accuracy)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Jw0iicynVVHi"
   },
   "outputs": [],
   "source": [
    "def comp_accuracy(model, data_loader):\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    num_examples = 0\n",
    "    \n",
    "    # turn on eval mode if model Inherits from nn.Module\n",
    "    if isinstance(model, nn.Module):\n",
    "        model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_index, (features, labels) in enumerate(data_loader):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predictions = torch.max(logits, dim=1) # single class with highest probability. simply retain indices\n",
    "\n",
    "            num_examples += labels.size(0)\n",
    "\n",
    "            correct += (predictions == labels).sum().float()\n",
    "            wrong += (predictions != labels).sum().float()\n",
    "            \n",
    "        accuracy = correct / num_examples * 100      \n",
    "        \n",
    "    return correct, wrong, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3LL2HIlVVHj"
   },
   "source": [
    "## 6. Training procedure\n",
    "- Training will be done on GPU if available\n",
    "- Everything else will remain the same as in the linear regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_WYWQC0OVVHj"
   },
   "outputs": [],
   "source": [
    "def fit(model, train_loader, epochs, learning_rate, loss_func=nn.CrossEntropyLoss(), opt_func=torch.optim.SGD):\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), learning_rate) # objective function\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model = model.train()\n",
    "              \n",
    "        for batch_index, (features, labels) in enumerate(train_loader):\n",
    "            \n",
    "            # gpu usage if possible\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 1. forward\n",
    "            logits = model(features)\n",
    "\n",
    "            # 2. compute objective function (softmax, cross entropy)\n",
    "            cost = loss_func(logits, labels)\n",
    "\n",
    "            # 3. cleaning gradients\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            # 4. accumulate partial derivatives\n",
    "            cost.backward() \n",
    "\n",
    "            # 5. step in the opposite direction of the gradient\n",
    "            optimizer.step() \n",
    "            \n",
    "            if not batch_index % 250:\n",
    "                print ('Epoch: {}/{} | Batch {}/{} | Cost: {:.4f}'.format(\n",
    "                    epoch+1,\n",
    "                    epochs,\n",
    "                    batch_index,\n",
    "                    len(train_loader),\n",
    "                    cost\n",
    "                ))\n",
    "        \n",
    "        correct, wrong, accuracy = comp_accuracy(model, train_loader)\n",
    "        print ('Training: Correct[{:.0f}] | Wrong[{:.0f}] | Accuracy[{:.2f}%]'.format(\n",
    "            correct,\n",
    "            wrong,\n",
    "            accuracy\n",
    "        ), '\\n')\n",
    "\n",
    "    correct, wrong, accuracy = comp_accuracy(model, test_loader)  \n",
    "    print ('Test: Correct[{:.0f}] | Wrong[{:.0f}] | Accuracy[{:.2f}%]'.format(\n",
    "        correct,\n",
    "        wrong,\n",
    "        accuracy\n",
    "    ), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nP8h1OkVVHj"
   },
   "source": [
    "## 1. Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "D_gL-SSaVVHk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Batch 0/1200 | Cost: 2.3632\n",
      "Epoch: 1/10 | Batch 250/1200 | Cost: 0.9561\n",
      "Epoch: 1/10 | Batch 500/1200 | Cost: 0.7735\n",
      "Epoch: 1/10 | Batch 750/1200 | Cost: 0.8270\n",
      "Epoch: 1/10 | Batch 1000/1200 | Cost: 0.7934\n",
      "Training: Correct[49300] | Wrong[10700] | Accuracy[82.17%] \n",
      "\n",
      "Epoch: 2/10 | Batch 0/1200 | Cost: 0.5931\n",
      "Epoch: 2/10 | Batch 250/1200 | Cost: 0.6580\n",
      "Epoch: 2/10 | Batch 500/1200 | Cost: 0.6855\n",
      "Epoch: 2/10 | Batch 750/1200 | Cost: 0.6615\n",
      "Epoch: 2/10 | Batch 1000/1200 | Cost: 0.5434\n",
      "Training: Correct[50750] | Wrong[9250] | Accuracy[84.58%] \n",
      "\n",
      "Epoch: 3/10 | Batch 0/1200 | Cost: 0.5120\n",
      "Epoch: 3/10 | Batch 250/1200 | Cost: 0.3446\n",
      "Epoch: 3/10 | Batch 500/1200 | Cost: 0.3999\n",
      "Epoch: 3/10 | Batch 750/1200 | Cost: 0.3637\n",
      "Epoch: 3/10 | Batch 1000/1200 | Cost: 0.4108\n",
      "Training: Correct[51524] | Wrong[8476] | Accuracy[85.87%] \n",
      "\n",
      "Epoch: 4/10 | Batch 0/1200 | Cost: 0.2805\n",
      "Epoch: 4/10 | Batch 250/1200 | Cost: 0.4766\n",
      "Epoch: 4/10 | Batch 500/1200 | Cost: 0.6631\n",
      "Epoch: 4/10 | Batch 750/1200 | Cost: 0.5480\n",
      "Epoch: 4/10 | Batch 1000/1200 | Cost: 0.4276\n",
      "Training: Correct[51828] | Wrong[8172] | Accuracy[86.38%] \n",
      "\n",
      "Epoch: 5/10 | Batch 0/1200 | Cost: 0.3750\n",
      "Epoch: 5/10 | Batch 250/1200 | Cost: 0.3934\n",
      "Epoch: 5/10 | Batch 500/1200 | Cost: 0.3418\n",
      "Epoch: 5/10 | Batch 750/1200 | Cost: 0.3765\n",
      "Epoch: 5/10 | Batch 1000/1200 | Cost: 0.2514\n",
      "Training: Correct[52372] | Wrong[7628] | Accuracy[87.29%] \n",
      "\n",
      "Epoch: 6/10 | Batch 0/1200 | Cost: 0.3104\n",
      "Epoch: 6/10 | Batch 250/1200 | Cost: 0.4367\n",
      "Epoch: 6/10 | Batch 500/1200 | Cost: 0.3976\n",
      "Epoch: 6/10 | Batch 750/1200 | Cost: 0.3738\n",
      "Epoch: 6/10 | Batch 1000/1200 | Cost: 0.4016\n",
      "Training: Correct[52602] | Wrong[7398] | Accuracy[87.67%] \n",
      "\n",
      "Epoch: 7/10 | Batch 0/1200 | Cost: 0.2555\n",
      "Epoch: 7/10 | Batch 250/1200 | Cost: 0.6328\n",
      "Epoch: 7/10 | Batch 500/1200 | Cost: 0.2825\n",
      "Epoch: 7/10 | Batch 750/1200 | Cost: 0.2438\n",
      "Epoch: 7/10 | Batch 1000/1200 | Cost: 0.3019\n",
      "Training: Correct[52967] | Wrong[7033] | Accuracy[88.28%] \n",
      "\n",
      "Epoch: 8/10 | Batch 0/1200 | Cost: 0.4088\n",
      "Epoch: 8/10 | Batch 250/1200 | Cost: 0.2728\n",
      "Epoch: 8/10 | Batch 500/1200 | Cost: 0.4205\n",
      "Epoch: 8/10 | Batch 750/1200 | Cost: 0.2822\n",
      "Epoch: 8/10 | Batch 1000/1200 | Cost: 0.3279\n",
      "Training: Correct[53059] | Wrong[6941] | Accuracy[88.43%] \n",
      "\n",
      "Epoch: 9/10 | Batch 0/1200 | Cost: 0.3209\n",
      "Epoch: 9/10 | Batch 250/1200 | Cost: 0.2285\n",
      "Epoch: 9/10 | Batch 500/1200 | Cost: 0.3732\n",
      "Epoch: 9/10 | Batch 750/1200 | Cost: 0.2451\n",
      "Epoch: 9/10 | Batch 1000/1200 | Cost: 0.4816\n",
      "Training: Correct[53260] | Wrong[6740] | Accuracy[88.77%] \n",
      "\n",
      "Epoch: 10/10 | Batch 0/1200 | Cost: 0.3894\n",
      "Epoch: 10/10 | Batch 250/1200 | Cost: 0.2477\n",
      "Epoch: 10/10 | Batch 500/1200 | Cost: 0.3803\n",
      "Epoch: 10/10 | Batch 750/1200 | Cost: 0.3589\n",
      "Epoch: 10/10 | Batch 1000/1200 | Cost: 0.3769\n",
      "Training: Correct[53461] | Wrong[6539] | Accuracy[89.10%] \n",
      "\n",
      "Test: Correct[8700] | Wrong[1300] | Accuracy[87.00%] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "model = MLP(784, 200, 100, 10)\n",
    "train_loader, test_loader = dloaders(batch_size=batch_size) # data iters\n",
    "fit(model, train_loader, epochs, learning_rate) # training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCa6fL8wzggw"
   },
   "source": [
    "### Summary 1. Attempt with Problem 1 - Part B Parameters\n",
    "\n",
    "With the hyper parameters from Problem 1 - Part B, our model achieves an accuracy of ~87 percent. It therefore provides approx. 4 percent better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieyDie-ndvgB"
   },
   "source": [
    "## 2. Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9DEqvaNzjgUI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Batch 0/1200 | Cost: 2.3516\n",
      "Epoch: 1/10 | Batch 250/1200 | Cost: 0.5006\n",
      "Epoch: 1/10 | Batch 500/1200 | Cost: 0.5191\n",
      "Epoch: 1/10 | Batch 750/1200 | Cost: 0.5828\n",
      "Epoch: 1/10 | Batch 1000/1200 | Cost: 0.5477\n",
      "Training: Correct[50608] | Wrong[9392] | Accuracy[84.35%] \n",
      "\n",
      "Epoch: 2/10 | Batch 0/1200 | Cost: 0.4515\n",
      "Epoch: 2/10 | Batch 250/1200 | Cost: 0.3562\n",
      "Epoch: 2/10 | Batch 500/1200 | Cost: 0.3796\n",
      "Epoch: 2/10 | Batch 750/1200 | Cost: 0.2359\n",
      "Epoch: 2/10 | Batch 1000/1200 | Cost: 0.4101\n",
      "Training: Correct[52048] | Wrong[7952] | Accuracy[86.75%] \n",
      "\n",
      "Epoch: 3/10 | Batch 0/1200 | Cost: 0.3070\n",
      "Epoch: 3/10 | Batch 250/1200 | Cost: 0.4326\n",
      "Epoch: 3/10 | Batch 500/1200 | Cost: 0.6614\n",
      "Epoch: 3/10 | Batch 750/1200 | Cost: 0.3928\n",
      "Epoch: 3/10 | Batch 1000/1200 | Cost: 0.2975\n",
      "Training: Correct[52596] | Wrong[7404] | Accuracy[87.66%] \n",
      "\n",
      "Epoch: 4/10 | Batch 0/1200 | Cost: 0.3212\n",
      "Epoch: 4/10 | Batch 250/1200 | Cost: 0.3847\n",
      "Epoch: 4/10 | Batch 500/1200 | Cost: 0.3021\n",
      "Epoch: 4/10 | Batch 750/1200 | Cost: 0.4072\n",
      "Epoch: 4/10 | Batch 1000/1200 | Cost: 0.3172\n",
      "Training: Correct[53239] | Wrong[6761] | Accuracy[88.73%] \n",
      "\n",
      "Epoch: 5/10 | Batch 0/1200 | Cost: 0.3514\n",
      "Epoch: 5/10 | Batch 250/1200 | Cost: 0.3278\n",
      "Epoch: 5/10 | Batch 500/1200 | Cost: 0.4297\n",
      "Epoch: 5/10 | Batch 750/1200 | Cost: 0.4221\n",
      "Epoch: 5/10 | Batch 1000/1200 | Cost: 0.1912\n",
      "Training: Correct[53366] | Wrong[6634] | Accuracy[88.94%] \n",
      "\n",
      "Epoch: 6/10 | Batch 0/1200 | Cost: 0.3049\n",
      "Epoch: 6/10 | Batch 250/1200 | Cost: 0.2534\n",
      "Epoch: 6/10 | Batch 500/1200 | Cost: 0.1928\n",
      "Epoch: 6/10 | Batch 750/1200 | Cost: 0.3529\n",
      "Epoch: 6/10 | Batch 1000/1200 | Cost: 0.2181\n",
      "Training: Correct[53803] | Wrong[6197] | Accuracy[89.67%] \n",
      "\n",
      "Epoch: 7/10 | Batch 0/1200 | Cost: 0.2148\n",
      "Epoch: 7/10 | Batch 250/1200 | Cost: 0.3341\n",
      "Epoch: 7/10 | Batch 500/1200 | Cost: 0.2581\n",
      "Epoch: 7/10 | Batch 750/1200 | Cost: 0.3007\n",
      "Epoch: 7/10 | Batch 1000/1200 | Cost: 0.3162\n",
      "Training: Correct[53469] | Wrong[6531] | Accuracy[89.11%] \n",
      "\n",
      "Epoch: 8/10 | Batch 0/1200 | Cost: 0.4711\n",
      "Epoch: 8/10 | Batch 250/1200 | Cost: 0.3082\n",
      "Epoch: 8/10 | Batch 500/1200 | Cost: 0.3496\n",
      "Epoch: 8/10 | Batch 750/1200 | Cost: 0.2969\n",
      "Epoch: 8/10 | Batch 1000/1200 | Cost: 0.2654\n",
      "Training: Correct[53966] | Wrong[6034] | Accuracy[89.94%] \n",
      "\n",
      "Epoch: 9/10 | Batch 0/1200 | Cost: 0.2863\n",
      "Epoch: 9/10 | Batch 250/1200 | Cost: 0.3063\n",
      "Epoch: 9/10 | Batch 500/1200 | Cost: 0.2561\n",
      "Epoch: 9/10 | Batch 750/1200 | Cost: 0.1492\n",
      "Epoch: 9/10 | Batch 1000/1200 | Cost: 0.2600\n",
      "Training: Correct[54338] | Wrong[5662] | Accuracy[90.56%] \n",
      "\n",
      "Epoch: 10/10 | Batch 0/1200 | Cost: 0.3438\n",
      "Epoch: 10/10 | Batch 250/1200 | Cost: 0.3790\n",
      "Epoch: 10/10 | Batch 500/1200 | Cost: 0.3293\n",
      "Epoch: 10/10 | Batch 750/1200 | Cost: 0.2365\n",
      "Epoch: 10/10 | Batch 1000/1200 | Cost: 0.3490\n",
      "Training: Correct[54329] | Wrong[5671] | Accuracy[90.55%] \n",
      "\n",
      "Test: Correct[8736] | Wrong[1264] | Accuracy[87.36%] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loader, test_loader = dloaders(batch_size=batch_size) # data iters\n",
    "model = MLP(784, 200, 100, 10)\n",
    "fit(model, train_loader, epochs, learning_rate) # training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GknKXKr509Ys"
   },
   "source": [
    "### Summary 2. Attempt, testing different parameters\n",
    "After playing with the hyper-parameters we have found that these:\n",
    "*   batch_size = 50\n",
    "*   epochs = 10\n",
    "*   learning_rate = 0.1\n",
    "\n",
    "gave us the best results.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLP (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
