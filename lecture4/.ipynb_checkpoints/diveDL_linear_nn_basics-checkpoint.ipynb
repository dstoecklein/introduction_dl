{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c648ee86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0868e8c",
   "metadata": {},
   "source": [
    "### Linear Regression Basics\n",
    "- Linear and Softmax regression can be casted to linear neural networks\n",
    "- Assumption: The relationship between the independent variables **x** and the dependent variable **y** is linear\n",
    "- The linearity assumpetion says, that the **label** (price) ca be expressed as a **weighted** sum of the **features** (size, year of renovation)\n",
    "\n",
    " $$\\mathcal{price} = (w_{size} * size) + (w_{year} * year) + b$$\n",
    "\n",
    "- The weights determine the influence of the feature on the prediction\n",
    "- The **bias** says, what value the predicted price should have, when all of the features take value 0\n",
    "- - Even if Size = 0 never happens, we need the bias or else we will imit the expressivity of our model\n",
    "\n",
    "- **Goal =** choose weights and the bias such that on average, the predictions made best fit the true prices (labels) on our data.\n",
    "![Drag Racing](whiteboard\\linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303226f",
   "metadata": {},
   "source": [
    "### Linear Regression in ML\n",
    "- Since in ML we work with high dimensional datasets\n",
    "- When input consists of ${d}$ features, we express the prediction $\\hat{y}$\n",
    "\n",
    "$$\\hat{y} = w_{1}x_{1} + ... + w_{d}x_{d} + b$$\n",
    "\n",
    "- collecting into **vectors**\n",
    "- - features $\\mathbf{x} \\in \\mathbb{R}^{d}$ \n",
    "- - weights $\\mathbf{w} \\in \\mathbb{R}^{d}$\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "- This Vector corresponds to features of a single data example\n",
    "- - It is better to refer to features of entire dataset of $n$\n",
    "- For a collection of features we use matrix $\\matbf{X}\n",
    "\n",
    "$$\\hat{y} = \\mathbf{X}\\mathbf{w} + b$$\n",
    "\n",
    "**Goal =** Given features of a training set $\\mathbf{X}$ known labels $\\mathbf{y}$. Find the weight vector $\\mathbf{w}$ and the bias term $b$. The new examples will be predicted with the lowest error.\n",
    "\n",
    "![Drag Racing](whiteboard\\scalar-vector-matrix.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba73071",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Loss Function - Least Squared Error\n",
    "- A measure of *fitness* - qualitfy measure of our model\n",
    "- Quantifies the *real* and the *predicted* value of the target\n",
    "- Usually a non-negative number, where **smaller = better** and **perfect = 0**\n",
    "- - $i$ = example\n",
    "- - $y^i$ = true label\n",
    "- - $\\hat{y}^i$ = prediction\n",
    "![Drag Racing](whiteboard\\least_squared.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89886f01",
   "metadata": {},
   "source": [
    "### Making Predictions with the learned Model\n",
    "- Given the learned linear model $\\mathbf{\\hat{w}}^{T}\\mathbf{x} + \\hat{b}$, we can now predict price of new houses\n",
    "- Give its size $x_{1}$ and year $x_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfbf052",
   "metadata": {},
   "source": [
    "### Normal Distribution (Normalverteilung) and Squared Loss\n",
    "- values are normaly distributed around the mean\n",
    "- mean = durchschnitt\n",
    "- std = standardabweichung\n",
    "![normal_distr](whiteboard\\normal_distribution.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc97b5",
   "metadata": {},
   "source": [
    "### From Linear Regression to Deep Networks\n",
    "- Neural Networks cover a much richer family of models\n",
    "- Now we rewrite things in a *layer* notation\n",
    "\n",
    "- Inputs = $x_{1}, ..., x_{d}$, so the number of inputs (*feature dimensionality*) is $d$\n",
    "- Output = $o_{1}$, so the number of Outputs is 1\n",
    "\n",
    "![normal_distr](whiteboard\\singleneuron.svg)\n",
    "\n",
    "- Linear regression models as neural networks consists of just a single neuron or as *single-layer neural networks*\n",
    "- Since in linear regression, every input is connected to every output = it is a *fully-connected layer* or **dense layer**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
