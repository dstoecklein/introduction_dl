{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent neural network (RNN) architectures\n",
    "\n",
    "(Built on section 8 of Zhang, A., Lipton, Z. C., Li, M. & Smola, A. J. Dive into Deep Learning. 2021. https://d2l.ai/)\n",
    "\n",
    "\n",
    "Latent variable models for sequences\n",
    "\n",
    "$$\n",
    "P(x_t | x_{t-1}, \\ldots, x_1) \\approx P(x_t | h_{t-1}), \\qquad\n",
    "h_t = f(x_t, h_{t-1})\n",
    "$$\n",
    "\n",
    "$h_t$: hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLP\n",
    "\n",
    "$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$: minibatch of $n$ examples (instances) of d-dimensional iputs\n",
    "\n",
    "$\\mathbf{H} \\in \\mathbb{R}^{n \\times m}$: hidden layer for minibatch of $n$ instances with m dimensions\n",
    "\n",
    "$\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$: output layer for minibatch of $n$ instances with q dimensions (e.g. regression $q=1$, 10-wise classifiction $q=10$)\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{XW}_{dm} + \\mathbf{b}_m)$$\n",
    "\n",
    "$\\mathbf{W}_{dm}$: weight matrix, $\\mathbf{b}_m$: bias vector\n",
    "\n",
    "$$\\mathbf{O} = \\mathbf{HW}_{mq} + \\mathbf{b}_q$$\n",
    "\n",
    "$\\mathbf{W}_{mq}$: weight matrix, $\\mathbf{b}_q$: bias vector\n",
    "\n",
    "![Mlp](mlp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN\n",
    "\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$: minibatch of $n$ examples (instances) of d-dimensional iputs **at time step $t$**\n",
    "\n",
    "$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times m}$: hidden variable (state) for minibatch of $n$ instances with m dimensions **at time step $t$**\n",
    "\n",
    "$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$: output layer for minibatch of $n$ instances with q dimensions  **at time step $t$** (e.g. regression $q=1$, 10-wise classifiction $q=10$)\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{dm} + \\mathbf{H}_{t-1} \\mathbf{W}_{mm} + \\mathbf{b}_m)$$\n",
    "\n",
    "$\\mathbf{W}_{dm}$, $\\mathbf{W}_{mm}$: weight matices, $\\mathbf{b}_m$: bias vector\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{mq} + \\mathbf{b}_q$$\n",
    "\n",
    "$\\mathbf{W}_{mq}$: weight matrix, $\\mathbf{b}_q$: bias vector\n",
    "\n",
    "![Rnn](rnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Character prediction\n",
    "\n",
    "Predict the next character based on the previous.\n",
    "\n",
    "Batch size $n=1$, inputs sequence \"machine\" tokenized to *characters* (26-dimensional one-hot vecgtors).\n",
    "\n",
    "\n",
    "![Rnn-character](rnn-train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation\n",
    "\n",
    "* organize **inputs** $\\mathbf{X}$ as (number of sequence steps, batch size, vocabulary size) - easy to loop over sequence steps\n",
    "* initiate shared **parameters** for hidden layer $\\mathbf{W}_{dm}, \\mathbf{W}_{mm}, \\mathbf{b}_m$ and output $\\mathbf{W}_{mq}, \\mathbf{b}_q$\n",
    "* initiate **hidden state** $H_0$ as zeros\n",
    "* **rnn** - loop over steps\n",
    "    * update hidden state $\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{dm} + \\mathbf{H}_{t-1} \\mathbf{W}_{mm} + \\mathbf{b}_m)$\n",
    "    * get output $\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{mq} + \\mathbf{b}_q$\n",
    "* **loss**: cross entropy averaged over all steps of sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Few practical tricks\n",
    "\n",
    "* **prefix** insert few initial tokens to *warm-up* hidden state\n",
    "* **clip gradients** to avoid exploding gradient due to multiplications in backpropagation\n",
    "$\\mathbf{g} \\leftarrow \\min (1, \\frac{\\theta}{||\\mathbf{g}||}\\mathbf{g})$\n",
    "* training over ordered batches - keep states from prevoius batch as $H_0$ but detach to avoid backpropagation\n",
    "* training with suffled batches - initiate $H_0$ for each batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: torch.Size([1, 32, 256])\n"
     ]
    }
   ],
   "source": [
    "# pytorch implementation\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# rnn layer\n",
    "hidden_dim = 256\n",
    "vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "input_dim = len(vocab)\n",
    "rnn_layer = nn.RNN(input_dim, hidden_dim)\n",
    "\n",
    "# init hidden state\n",
    "rnn_layers = 1\n",
    "batch_size = 32\n",
    "state = torch.zeros((rnn_layers, batch_size, hidden_dim))\n",
    "print(f'State: {state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([10, 32, 8])\n",
      "out: torch.Size([10, 32, 256]), state: torch.Size([1, 32, 256])\n"
     ]
    }
   ],
   "source": [
    "# pass data through rnn layer\n",
    "\n",
    "# get random X\n",
    "num_steps = 10\n",
    "X = torch.randint(len(vocab), size=(num_steps, batch_size))\n",
    "X = F.one_hot(X).float()\n",
    "print(f'X: {X.shape}')\n",
    "\n",
    "# pass X through rnn\n",
    "out, state_new = rnn_layer(X, state)  # out same as state, can be passed to another rnn layer or to output func\n",
    "print(f'out: {out.shape}, state: {state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation through time\n",
    "\n",
    "* Backpropagation as you know it through sequential model\n",
    "* Expand computational graph of RNN through the sequence steps\n",
    "\n",
    "### Unrolling computational graph\n",
    "\n",
    "![Rnn-character](rnn-bptt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation through time - math\n",
    "\n",
    "Simplified notation:\n",
    "$$h_t = f(x_t, h_{t-1}, w_h) \\qquad o_t = g(h_t, w_o)$$\n",
    "\n",
    "After running RNN forward we have:\n",
    "$$[(x_1, h_1, o_1), (x_2, h_2, o_2), \\ldots, (x_T, h_T, o_T)]$$\n",
    "\n",
    "Objective function:\n",
    "$$L(x_1, \\ldots, x_T, y_1, \\ldots, y_T, o_1, \\ldots, o_T, w_h, w_o) = \\frac{1}{T}\\sum_{t=1}^{T} l(y_t, o_t)$$\n",
    "\n",
    "Backpropagation via chain rule:\n",
    "$$\\frac{\\partial L}{\\partial w_h} = \\frac{1}{T}\\sum_{t=1}^{T} \\frac{\\partial l(y_t, o_t)}{\\partial w_h}\n",
    "= \\frac{1}{T}\\sum_{t=1}^{T} \\frac{\\partial l(y_t, o_t)}{\\partial o_t} \\frac{\\partial g(h_t, w_o)}{\\partial h_t}\n",
    "\\frac{\\partial h_t}{\\partial w_h}\n",
    "$$\n",
    "\n",
    "**However**:\n",
    "$$\\frac{\\partial h_t}{\\partial w_h} = \\frac{\\partial f(x_t, h_{t-1}, w_h)}{\\partial w_h} + \\frac{\\partial f(x_t, h_{t-1}, w_h)}{\\partial h_{t-1}}\\frac{\\partial h_{t-1}}{\\partial w_h} \\\\\n",
    "\\frac{\\partial h_{t-1}}{\\partial w_h} = \\frac{\\partial f(x_t, h_{t-2}, w_h)}{\\partial w_h} + \\frac{\\partial f(x_t, h_{t-2}, w_h)}{\\partial h_{t-2}}\\frac{\\partial h_{t-2}}{\\partial w_h} \\\\\n",
    "\\ldots\n",
    "$$\n",
    "\n",
    "**Full recurrence**:\n",
    "$$\\frac{\\partial h_t}{\\partial w_h} = \\frac{\\partial f(x_t, h_{t-1}, w_h)}{\\partial w_h} +\n",
    "\\sum_{i=1}^{t-1} \\left( \\prod_{j=i+1}^t \\frac{\\partial f(x_j, h_{j-1}, w_h)}{\\partial h_{j-1}} \\right) \\frac{\\partial f(x_i, h_{i-1}, w_h)}{\\partial w_h}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vanishing / exploding gradients\n",
    "\n",
    "* products of many terms (many gradients): if terms $<1$ then product $\\to 0$; if terms $>1$ then product $\\to \\infty$;\n",
    "\n",
    "### Truncated backpropagation through time\n",
    "\n",
    "* truncate backdward grad calculation to just a few steps (terminate with $\\partial h_{t-\\tau} / \\partial w_h$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modern RNN architectures (LSTM/GRU)\n",
    "\n",
    "* **RNN major issue: numerical instability of bptt (vanishing / exploding gradients)**\n",
    "* more sophisticated design\n",
    "\n",
    "### Wish list\n",
    "* important piece of info in the beginning of sequence:\n",
    "    * a) large gradient to impact all future (effect on multiplication)\n",
    "    * b) **memory cell** to store vital info for later\n",
    "\n",
    "* some tokens carry no info\n",
    "    * a) small gradient (effect on multiplication)\n",
    "    * b) **skipping mechanism**\n",
    "    \n",
    "* logical break in sequence\n",
    "    * a) prevent passing of gradient\n",
    "    * b) **resetting mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gated Recurrent Units (GRU)\n",
    "\n",
    "Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine\n",
    "translation: encoder-decoder approaches. arXiv preprint arXiv:1409.1259\n",
    "\n",
    "### 1) Reset and update gates\n",
    "\n",
    "![Rnn-character](gru-1.png)\n",
    "\n",
    "minibatch of inputs $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$, previous hidden state $\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$, *reset gate* $\\mathbf{R}_t \\in \\mathbb{R^{n \\times h}}$, *update gate* $\\mathbf{Z}_t \\in \\mathbb{R^{n \\times h}}$ \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{dh}^{(r)} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}^{(r)} + \\mathbf{b}_h^{(r)}),\\\\\n",
    "\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{dh}^{(z)} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}^{(z)} + \\mathbf{b}_h^{(z)}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### sigmoid nonlinearity $\\to \\mathbf{R}_t, \\mathbf{Z}_t \\in (0, 1)$ !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2) Candidate hidden state\n",
    "\n",
    "![Rnn-character](gru-2.png)\n",
    "\n",
    "*candidate hidden state*\n",
    "$\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$ at time step $t$ - *effect of reset on hidden state $\\mathbf{R}_t$*\n",
    "\n",
    "$$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{dh}^{(h)} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh}^{(h)} + \\mathbf{b}_h^{(h)}),$$\n",
    "\n",
    "$\\odot$ - elementwise product\n",
    "\n",
    "### tanh nonlinearity $\\to \\tilde{\\mathbf{H}}_t \\in (-1, 1)$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3) Hidden state\n",
    "\n",
    "![Rnn-character](gru-3.png)\n",
    "\n",
    "*effect of update gate $\\mathbf{Z}_t$* - what comes from the *old* $\\mathbf{H}_t$ and what from the new *new candidate* $\\tilde{\\mathbf{H}}_t$ - elementwise convex combination of the two.\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$\n",
    "\n",
    "\n",
    "* when update gate $\\mathbf{Z}_t$ close to 1: retain old state $\\mathbf{H}_t$ and ingnore new token $\\mathbf{X}_t$\n",
    "* when $\\mathbf{Z}_t$ close to 0: new hidden $\\mathbf{H}_t$ mainly uses candidate state $\\tilde{\\mathbf{H}}_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(8, 256)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch imlementation - trivial :)\n",
    "\n",
    "nn.GRU(input_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)\n",
    "\n",
    "Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735–1780.\n",
    "\n",
    "Much older then GRUs a quite a bit more complicated\n",
    "\n",
    "**memory cell**: ouptut gate, input gate, foreget gate\n",
    "\n",
    "![lstm](lstm-3.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
