{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc6a67a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN architectures\n",
    "\n",
    "## History of significant breakthroughs\n",
    "\n",
    "* implementing DNN is easy (stack layers)\n",
    "* what architecture? hyperparams?\n",
    "* intuition, math insights, trial and error, luck, time, effort, frustration, occasional success\n",
    "\n",
    "\n",
    "Based on Zhang, Aston, Zachary C. Lipton, Mu Li, and Alexander J. Smola. ‘Dive into Deep Learning’. ArXiv Preprint ArXiv:2106.11342, 2021.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46faba7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LeNet - the grandfathers\n",
    "\n",
    "LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., & others. (**1998**). Gradient-based learning applied to\n",
    "document recognition. Proceedings of the IEEE, 86(11), 2278–2324.\n",
    "\n",
    "* Yann LeCun, Leon Bottou, Yoshua Bengio - names to remember!\n",
    "* decade of research to get here\n",
    "* first successful CNN with backprop\n",
    "* handwritten digits recognition (MNIST dataset)\n",
    "* outstanidng results matching SVMs :)  (~99.05 test set accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c238db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Yoshua Bengio, Geoff Hinton and Yann LeCun.](BengioHintonLeCun2.jpeg)\n",
    "\n",
    "Yoshua Bengio - Uni Montreal and Mila, Geoff Hinton - Uni Toronto, and Yann LeCun - Facebook and Uni NY (born French :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc1174",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LeNet-5 architecture\n",
    "\n",
    "![LeNet for handwritten digit recongnition.](lenet.svg)\n",
    "\n",
    "* convolutional block = convolutional layer + sigmoid activation (ReLU not yet invented then!)\n",
    "* subsampling average pooling (max pooling not yet used!)\n",
    "* convolutions: 5x5 kernels, 1st - 6 output channels, padding 2. 2nd - 16 channels, padding 0\n",
    "* poling: 2x2 with stride 2\n",
    "* 3 dense layers: 120, 84, 10 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca458681",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternative depiction\n",
    "\n",
    "![LeNet schematic depiction.](lenet-vert.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f3dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet PyTorch implementation\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "\n",
    "lenet = nn.Sequential(\n",
    "  nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "  nn.Sigmoid(),\n",
    "  nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "  nn.Conv2d(6, 16, kernel_size=5),\n",
    "  nn.Sigmoid(),\n",
    "  nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "  nn.Flatten(),\n",
    "  nn.Linear(16*5*5, 120),\n",
    "  nn.Sigmoid(),\n",
    "  nn.Linear(120, 84),\n",
    "  nn.Sigmoid(),\n",
    "  nn.Linear(84, 10),\n",
    "  nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4ad1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (1): Sigmoid()\n",
      "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): Sigmoid()\n",
      "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (6): Flatten()\n",
      "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (8): Sigmoid()\n",
      "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (10): Sigmoid()\n",
      "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (12): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40fdece3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape: \t torch.Size([5, 6, 28, 28])\n",
      "Sigmoid output shape: \t torch.Size([5, 6, 28, 28])\n",
      "AvgPool2d output shape: \t torch.Size([5, 6, 14, 14])\n",
      "Conv2d output shape: \t torch.Size([5, 16, 10, 10])\n",
      "Sigmoid output shape: \t torch.Size([5, 16, 10, 10])\n",
      "AvgPool2d output shape: \t torch.Size([5, 16, 5, 5])\n",
      "Flatten output shape: \t torch.Size([5, 400])\n",
      "Linear output shape: \t torch.Size([5, 120])\n",
      "Sigmoid output shape: \t torch.Size([5, 120])\n",
      "Linear output shape: \t torch.Size([5, 84])\n",
      "Sigmoid output shape: \t torch.Size([5, 84])\n",
      "Linear output shape: \t torch.Size([5, 10])\n",
      "Sigmoid output shape: \t torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(5, 1, 28, 28))\n",
    "for layer in lenet:\n",
    "    X = layer(X)\n",
    "    print(f'{layer.__class__.__name__} output shape: \\t {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dd362",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What happened next?\n",
    "\n",
    "### Nothing! (well not quite)\n",
    "\n",
    "* 1990 - ~2010: \"traditional\" computer vision methods dominated NNs\n",
    "* state-of-the-art: hand-crafted features combined with standard ML classifier (SIFT, SURF, HOG + SVM, NN, DT) \n",
    "* ML researchers: ML methods are cool, elegant, mathematically well motivated\n",
    "* CV researchers: data are dirty, domain knowledge matters, ML methods are secondary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ccbe47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Heroes\n",
    "\n",
    "### Main idea: learn the features!\n",
    "\n",
    "* guerrila exploring alternatives\n",
    "*  Yann LeCun, Geoff Hinton, Yoshua Bengio, Andrew Ng, Shun-ichi Amari, Juergen Schmidhuber - more names to remember!\n",
    "\n",
    "![Ng Amari Schmidhuber](NgAmariSchmidhuber.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13e43c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Missing bits\n",
    "\n",
    "### Data\n",
    "\n",
    "* research in 1990-2010 based on tiny datasets (UCI repository) - anecdotal from kernel world: \"4000 instances is a BIG dataset worth special attention\"\n",
    "* 2009 wow! **ImageNet Challenge**: 1 million examples (3x224x224) = 1k each from 1k categories, Fei-Fei Li (remember!), Google Image Search, Amazon Mechanical Turk\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* DNN relies on compute many cycles of simple operations\n",
    "* GPUs developped for graphis in gaming\n",
    "* requiremens proved similar to what CNNs need\n",
    "* NVIDIA and ATM develop general purpose GPUs\n",
    "* CPUs vs GPUs hardware pros and cons - go to MAI :)\n",
    "* AlexNet - use GPUs for CNNs - breakthrough!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59ee64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AlexNet\n",
    "\n",
    "Krizhevsky, A., Sutskever, I., & Hinton, G. E. (**2012**). Imagenet classification with deep convolu-\n",
    "tional neural networks. Advances in neural information processing systems (pp. 1097–1105).\n",
    "\n",
    "*  Alex Krizhevsky (??), Ilya Sutskever (OpenAI founder) and Geoff Hinten - even more names to remember\n",
    "* let's use GPUs! wow! remember, no PyTorch (2017), no TensorFlow (2015) then!\n",
    "* finally **beating** the manually constructed features from 75% to 84.7% accuracy (ImageNet)\n",
    "\n",
    "![Krizhevsky and Sutsakaver](KrizhevskySutsakaver.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263f07c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LeNet vs AlexNet\n",
    "\n",
    "![LeNet vs AlexNet.](alexnet.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e356393",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LeNet vs AlexNet\n",
    "\n",
    "* input dimension much bigger\n",
    "* AlexNet **deeper**\n",
    "* ReLU instead of sigmoids\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "* 11x11 kernel to capture object in large image\n",
    "* reduce dimensionality rapidly (strides)\n",
    "* many more channels (10x as many)\n",
    "\n",
    "### Activation function - ReLU\n",
    "\n",
    "* simpler / faster to compute (no exponentiation)\n",
    "* less sensitive to initialization - sigmoid gradient 0 when activation close to 0/1\n",
    "\n",
    "### Complexity control - dropout, augmentation\n",
    "\n",
    "* **droppout**:  only relevant to fully connected layers\n",
    "* **data augmentation**: flipping, clipping, color changes: larger trainset size, smaller overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ada29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet PyTorch implementation\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "alexnet = nn.Sequential(\n",
    "    nn.Conv2d(3, 96, kernel_size=11, stride=4),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Conv2d(256, 384, kernel_size=5, padding=2),\n",
    "    nn.Conv2d(384, 384, kernel_size=5, padding=2),\n",
    "    nn.Conv2d(384, 256, kernel_size=5, padding=2),    \n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(6400, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 1000)    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ac5a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VGG net (Visual Geometry Group Oxford Uni)\n",
    "\n",
    "Simonyan, K., & Zisserman, A. (**2014**). Very deep convolutional networks for large-scale image\n",
    "recognition. arXiv preprint arXiv:1409.1556.\n",
    "\n",
    "AlexNet great - finally it works! But how do we develop other NNs?\n",
    "\n",
    "### Main idea: blocks\n",
    "\n",
    "Move from thinking about neurons, then layers to thinking about blocks of layers with repeating patterns\n",
    "\n",
    "* convolutional layer with padding to maintain spatial resolution (VGG 3x3 kernels, padding 1)\n",
    "* nonlinearity (ReLU and friends)\n",
    "* max pooling for downsampling (VGG 2x2, stride 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10cdd83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From AlexNet to VGG\n",
    "\n",
    "![LeNet vs AlexNet.](vgg.svg)\n",
    "\n",
    "VGG-11: 1 conv(64), 1 conv(128), 2 conv(256), 2 conv(512), 2 conv(512), FC(4096), FC(4096), FC(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a492115",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NiN - Network in Network (Singapore)\n",
    "\n",
    "\n",
    "Lin, M., Chen, Q., & Yan, S. (**2013**). Network in network. arXiv preprint arXiv:1312.4400.\n",
    "\n",
    "LeNet -> AlexNet -> VGG net: stack convolutional blocks and finish by FC layers, improvements from more width and depth\n",
    "\n",
    "### Main idea: use FC layers earlier\n",
    "\n",
    "Hmm ... but how not to lose the spatial info? -> **1x1 convolution**\n",
    "\n",
    "* FC layer acting at each pixel accross channels\n",
    "* each pixel an instance with channels as features\n",
    "* parameters tied through common kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc65009",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From VGG to NiN\n",
    "\n",
    "![Nin](nin.svg)\n",
    "\n",
    "No FC layers at the end! Last NiN blocks as many channels as classes follow by global avg pooling -> smaller num of parameters, longer train time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7968a16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GoogLeNet \n",
    "\n",
    "Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Rabinovich, A. (**2015**). Going deeper with convolutions. Proceedings of the IEEE conference on computer vision and pattern\n",
    "recognition.\n",
    "\n",
    "### Mian diea: combine variously-sized kernels into single block - Inception block\n",
    "\n",
    "\n",
    "![Nin](inception.svg)\n",
    "\n",
    "Preserves spatial dimension, main hyper-parameter is number of output channels (93.3% accuracy on ImageNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d9bed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GoogleNet architecture\n",
    "\n",
    "\n",
    "![inception-full](inception-full.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b895236",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batchnorm\n",
    "\n",
    "Ioffe, S., & Szegedy, C. (**2015**). Batch normalization: accelerating deep network training by reduc-\n",
    "ing internal covariate shift. arXiv preprint arXiv:1502.03167.\n",
    "\n",
    "### Main idea: deeper nets take long to train - we need to accelerate convergence through normalization\n",
    "\n",
    "* Remember: data preprocessing in linear regression net via normalization to **align scaling** - speed up convergece\n",
    "* Deep net: each layer output (next layer input) widely varying magnitues (across nodes, channels, layers)\n",
    "* Batch normalization idea: before each layer normalize each batch by its statistics (mean, std)\n",
    "* Careful: SGD with batch size=1 - batchnorm does nothing => batch size matters even more! (seems to work best for 50~100 range)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d78cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batchnorm math\n",
    "\n",
    "$\\mathbf{x} \\in \\mathcal{B}$ minibatch (input)\n",
    "\n",
    "$$\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}.$$\n",
    "\n",
    "* $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B} = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}$ - batch mean \n",
    "* $\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}^2 = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}})^2 + \\epsilon$ - batch variance (sqrt is standard deviation), small $\\epsilon$ to avoid division by 0\n",
    "* after applying these the new mean = 0, new std = 1\n",
    "* $\\boldsymbol{\\gamma}$ - elementwise scale parameter (learned)\n",
    "* $\\boldsymbol{\\beta}$ - elementwise shift parameter (learned)\n",
    "\n",
    "* Using noisy estimates $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ and ${\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}$ based on batches rather than full data - some randomness during training helps (think random perturbations, dropout).\n",
    "* At test time use full train-sample statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b74ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batchnorm in FC and CNN layers\n",
    "\n",
    "#### FC - after affine transform before nonlinearity\n",
    "\n",
    "The original paper suggested: $\\mathbf{h} = \\phi(\\mathrm{BN}(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) ).$\n",
    "\n",
    "#### CN - after convolution before nonlinearity, per each channel independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699660c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Controversy\n",
    "\n",
    "* Intuitively, batchnorm smoothes the optimization landscape. But does it really?\n",
    "* \"reducing *internal covariate shift*\". What the h... is that?\n",
    "* No, it actually does not. So how come it works?\n",
    "* Example of intuition that works in practice but theory lacks behind => disturbing\n",
    "* Alimi Rahimi NeurIPS2017: \"deep learning = alchemy\" :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24524de2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Residual Network (ResNet)\n",
    "\n",
    "He, K., Zhang, X., Ren, S., & Sun, J. (**2016**). Deep residual learning for image recognition. Proceed-\n",
    "ings of the IEEE conference on computer vision and pattern recognition (pp. 770–778).\n",
    "\n",
    "Questions to consider\n",
    "* adding layers increases complexity, but does it help ?\n",
    "* NN more expressive vs simply differnt?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559d2ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ResNet - some theory\n",
    "\n",
    "* Trfained NN - function $f : \\mathcal{X} \\to \\mathcal{Y}$\n",
    "* NN architecture - set of functions $f \\in \\mathcal{F}$ varying in parameter settings, hyperparameters, etc.\n",
    "* $f^*$ the \"best / truth\" function, typically $f^* \\notin \\mathcal{F}$\n",
    "* make $\\mathcal{F}$ more complex - any closer to $f^*$?\n",
    "* non-nested functions $\\approx$ no guarantee $\\Rightarrow$ **use nested functions**\n",
    "\n",
    "![inception-full](functionclasses.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694abdc",
   "metadata": {},
   "source": [
    "### ResNet in practice\n",
    "\n",
    "* construct layer that can train to $f(\\mathbf{x})=\\mathbf{x}$\n",
    "* *residual (shortcut) connection* propagates inputs fast ahead\n",
    "* residual block $f(\\mathbf{x})-\\mathbf{x}$ can be trained to zero in the weight layer\n",
    "\n",
    "![inception-full](residual-block.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7c5d7",
   "metadata": {},
   "source": [
    "### ResNet block architecture\n",
    "\n",
    "* follow from VGG net\n",
    "* two 3x3 convolutions with same number of output channels\n",
    "* batch norm and ReLU\n",
    "* preserve input dim through convolutions so can be added to the skip connection\n",
    "* but can change skip-connection channels through 1x1 convolution\n",
    "\n",
    "![inception-full](resnet-block.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9919e27",
   "metadata": {},
   "source": [
    "## ResNet-18 model\n",
    "\n",
    "\n",
    "![inception-full](resnet18.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10a0f0e",
   "metadata": {},
   "source": [
    "### ResNet layers\n",
    "\n",
    "* inspired by GoogLeNet\n",
    "* resolution decreases, channels increase\n",
    "* use BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bfe6ae",
   "metadata": {},
   "source": [
    "## Densely Connected Network (DenseNet)\n",
    "\n",
    "Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional\n",
    "networks. Proceedings of the IEEE conference on computer vision and pattern recognition\n",
    "(pp. 4700–4708).\n",
    "\n",
    "\n",
    "![densenet](densenet-block.svg)\n",
    "\n",
    "* difference from ResNet: concatenation (on channels) instead of addition\n",
    "* ResNet: $\\mathbf{x} \\to f_2(\\mathbf{x} + f_1(\\mathbf{x})) + \\mathbf{x} + f_1(\\mathbf{x})$\n",
    "* DenseNet: $\\mathbf{x} \\to [\\mathbf{x}, f_1(\\mathbf{x}), f2([\\mathbf{x}, f_1(\\mathbf{x})])]$\n",
    "* use 1x1 convolutions to reduce num of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d40fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
