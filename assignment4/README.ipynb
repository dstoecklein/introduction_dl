{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20e3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89f7d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4, 5])\n",
      "Parameter containing:\n",
      "tensor([[[[-0.1437,  0.1753, -0.0937, -0.1739, -0.0025],\n",
      "          [ 0.1373, -0.0530,  0.1809, -0.0898,  0.1127],\n",
      "          [ 0.0645,  0.0225,  0.1821,  0.0647,  0.0613],\n",
      "          [ 0.1500, -0.0325, -0.0146, -0.0353, -0.2124]]],\n",
      "\n",
      "\n",
      "        [[[-0.1619,  0.0244,  0.0756, -0.0175, -0.0116],\n",
      "          [ 0.1618, -0.1654,  0.0500, -0.1326,  0.2008],\n",
      "          [-0.2177,  0.0448,  0.2171,  0.1807,  0.1891],\n",
      "          [-0.0334,  0.1107,  0.0766, -0.0709,  0.1484]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1024,  0.0317, -0.2038,  0.0486,  0.1355],\n",
      "          [ 0.0859, -0.1986,  0.0337,  0.2104, -0.1045],\n",
      "          [ 0.0785, -0.1585,  0.1806, -0.1438, -0.1261],\n",
      "          [ 0.0785,  0.0375, -0.0441,  0.2103, -0.0319]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# It will produce 3 filters with the shape 4 rows, 5 columns\n",
    "c = nn.Conv2d(1, 3, stride=1, kernel_size=(4, 5))\n",
    "print(c.weight.shape)\n",
    "print(c.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd05849",
   "metadata": {},
   "source": [
    "## CNN\n",
    "### Reminder:\n",
    "- Input shape: $n_h\\times n_w$\n",
    "- Kernel shape: $k_h\\times k_w$,\n",
    "- Output shape: $(n_h-k_h+1) \\times (n_w-k_w+1)$. Since kernels are usually > 1, the output will be always smaller than input\n",
    "- Example: Input $240 \\times 240$ pixel image, $10$ layers of $5 \\times 5$ convolutions = $200 \\times 200$ pixel image\n",
    "- The ``out_channels`` is what convolution will produce so these are the **number of filters**. They are usually choosen by intuition.\n",
    "\n",
    "### Padding\n",
    "- Padding can handle this issue and affect the output size\n",
    "- It adds extra pixels around the boundary of the input image\n",
    "- padding=1, input=$3\\times 3$, kernel_size=2, output=$4 \\times 4$\n",
    "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$\n",
    "- **For the kernel size we usually use odd numbers such as 1,3,5,7 to keep the spatial dimensionality while padding.**\n",
    "\n",
    "### Stride\n",
    "- Sometimes we want to move our kernel-window more than 1 element\n",
    "- This has computational or downsample reasons\n",
    "- The first layer in the ResNet uses convolution with strides. This is a great example of when striding gives you an advantage. This layer by itself significantly reduces the amount of computation that has to be done by the network in the subsequent layers. It compresses multiple 3x3 convolution (3 to be exact) in to one 7x7 convolution, to make sure that it has exactly the same receptive field as 3 convolution layers (even though it is less powerful in terms of what it can learn).\n",
    "\n",
    "### Multiple Input Channels\n",
    "- When the input has multiple channels, we need to construct a conv kernel with the same number of input channels\n",
    "<img src=\"https://d2l.ai/_images/conv-multi-in.svg\">\n",
    "\n",
    "### 1x1 conv\n",
    "- Typically used to adjust the number of channels between network layers and to control model complexity.\n",
    "\n",
    "### Pooling\n",
    "- Downsample feature maps\n",
    "- Convolutional layers prove very effective, and stacking convolutional layers in deep models allows layers close to the input to learn low-level features (e.g. lines) and layers deeper in the model to learn high-order or more abstract features, like shapes or specific objects.\n",
    "- A limitation of the feature map output of convolutional layers is that they record the precise position of features in the input. This means that small movements in the position of the feature in the input image will result in a different feature map. This can happen with re-cropping, rotation, shifting, and other minor changes to the input image.\n",
    "- A common approach to addressing this problem from signal processing is called down sampling. This is where a lower resolution version of an input signal is created that still contains the large or important structural elements, without the fine detail that may not be as useful to the task.\n",
    "- Down sampling can be achieved with convolutional layers by changing the stride of the convolution across the image. A more robust and common approach is to use a pooling layer.\n",
    "- A pooling layer is a new layer added after the convolutional layer. Specifically, after a nonlinearity (e.g. ReLU) has been applied to the feature maps output by a convolutional layer; for example the layers in a model may look as follows:\n",
    "<img src=\"http://d2l.ai/_images/pooling.svg\">\n",
    "- Max pooling extracts the most important features like edges whereas, average pooling extracts features so smoothly.\n",
    "\n",
    "### Dropout\n",
    "- Faced with more features than examples, linear models tend to overfit\n",
    "- Regularization to prevent over-fitting.\n",
    "- Simply put, dropout refers to ignoring units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random.\n",
    "\n",
    "### Batch Normalization\n",
    "- Together with residual blocks—covered later in Section 7.6—batch normalization has made it possible for practitioners to routinely train networks with over 100 layers.\n",
    "- we can apply batch normalization after the convolution and before the nonlinear activation function.\n",
    "- BN after ReLU makes much more sense - the weight matrix W then looks at mean-centered data.\n",
    "- -> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a7989",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "- CNN orientierung at trainingsdata\n",
    "- Model classifies training data too good and test data too bad\n",
    "- Model is bad at generalization\n",
    "- Can be tested if training acc is high but test acc low\n",
    "- can be avoided:\n",
    "- - more training data\n",
    "- - reduce amount of features (not needed ones), less dimensions\n",
    "- - keep model simple\n",
    "- - regularization (dropout, weight decay)\n",
    "- - weight decay: Keep weights small. prevent large weight values. The larger the weight are, the larger the l2 norm will be "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
